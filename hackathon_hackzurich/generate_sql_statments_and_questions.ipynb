{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text to SQL - Questions generation and testing for the Statbot-Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/text-to-sql-learning-to-query-tables-with-natural-language-7d714e60a70d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "from typing import List, Tuple\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to add views to generate questions, you should add rows to the csv file in input_data/INDICATORS_VIEW.csv. Do not fill all columns (indicator_id, a short description, question type (as 1), name of the view and column name of the view value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "NB_SAMPLES = 1 # number of samples per type (default: 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Zurich and Basel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "INPUT_FOLDER = 'input_data'\n",
    "INDICATORS_FILE = 'EN_INDICATORS.csv'\n",
    "INDICATORS_VALUES_FILE = 'EN_INDICATOR_VALUES.csv'\n",
    "INDICATORS_VIEWS_FILE = 'INDICATORS_VIEWS.csv'\n",
    "SPATIALUNIT_FILE = 'EN_T_SPATIALUNIT.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation of natural language questions & SQL Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a dataset with indicator metadata and short descriptions which can be used to generate questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators = pd.read_csv(os.path.join(INPUT_FOLDER, INDICATORS_FILE))\n",
    "indicators.columns = [c.lower() for c in indicators.columns]\n",
    "indicators = indicators.rename(columns={\"name\": \"indicator_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = pd.read_csv(os.path.join(INPUT_FOLDER, INDICATORS_VIEWS_FILE))\n",
    "indicators = indicators.merge(descriptions, on='indicator_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single dataset with all information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a combined dataset containing all the information (values, indicator labels, spatial unit labels)  to make the generation of the question / sql pairs easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_values = pd.read_csv(os.path.join(INPUT_FOLDER, INDICATORS_VALUES_FILE))\n",
    "indicators_values.columns = [c.lower() for c in indicators_values.columns]\n",
    "gp_data = indicators_values.merge(indicators, on='indicator_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_units = pd.read_csv(os.path.join(INPUT_FOLDER, SPATIALUNIT_FILE))\n",
    "spatial_units.columns = [c.lower() for c in spatial_units.columns]\n",
    "spatial_units = spatial_units.rename(columns={'name': 'municipality_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_data = gp_data.merge(spatial_units, on='spatialunit_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset with random values, years and municipalities per indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To fill in the questions and queries, random values, years and municipalities are drawn for each indicator. These are then integrated into the templates dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_units = spatial_units[['spatialunit_id', 'municipality_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_data = gp_data[gp_data['type_id']==1]\n",
    "gp_data = gp_data[gp_data['question_type']<=2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one random sample of each group\n",
    "grouping_columns = ['indicator_id', 'indicator_name', 'short_description', 'question_type']\n",
    "grouped_samples = gp_data.groupby(by=grouping_columns)\n",
    "\n",
    "sample_columns = grouping_columns + ['value', 'year', 'spatialunit_id', 'view_name', 'view_column_value']\n",
    "samples = grouped_samples.sample(n=NB_SAMPLES).reset_index()[sample_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = samples.merge(spatial_units, on='spatialunit_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>indicator_id</th>\n",
       "      <th>indicator_name</th>\n",
       "      <th>short_description</th>\n",
       "      <th>question_type</th>\n",
       "      <th>value</th>\n",
       "      <th>year</th>\n",
       "      <th>spatialunit_id</th>\n",
       "      <th>view_name</th>\n",
       "      <th>view_column_value</th>\n",
       "      <th>municipality_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>385</td>\n",
       "      <td>Access by bus [% of inhabitants]</td>\n",
       "      <td>share of people living in proximity of a busstop</td>\n",
       "      <td>1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>2009</td>\n",
       "      <td>19</td>\n",
       "      <td>accessibility_bus</td>\n",
       "      <td>access_by_bus</td>\n",
       "      <td>Dachsen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>386</td>\n",
       "      <td>Access by suburban train [% of inhabitants]</td>\n",
       "      <td>share of people living in proximity of a train...</td>\n",
       "      <td>1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>2014</td>\n",
       "      <td>89</td>\n",
       "      <td>accessibility_train</td>\n",
       "      <td>access_by_suburban_train</td>\n",
       "      <td>Hinwil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>399</td>\n",
       "      <td>Passenger cars per 1000 inhabitants [no.]</td>\n",
       "      <td>number of cars per capita</td>\n",
       "      <td>2</td>\n",
       "      <td>539.9</td>\n",
       "      <td>2016</td>\n",
       "      <td>114</td>\n",
       "      <td>number_of_passenger_cars</td>\n",
       "      <td>passenger_cars_per_1000_inhabitants</td>\n",
       "      <td>Uetikon a.S.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>401</td>\n",
       "      <td>Accessibility by suburban train+bus [% of inha...</td>\n",
       "      <td>share of people living in proximity of a train...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2001</td>\n",
       "      <td>146</td>\n",
       "      <td>accessibility_train_and_bus</td>\n",
       "      <td>access_by_suburban_train_and_bus</td>\n",
       "      <td>Ellikon a.d.Th.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>480</td>\n",
       "      <td>public transport share (modal split) [%]</td>\n",
       "      <td>share of public transport in traffic movements...</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2016</td>\n",
       "      <td>155</td>\n",
       "      <td>public_transport_share</td>\n",
       "      <td>public_transport_share_modal_split</td>\n",
       "      <td>Seuzach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>481</td>\n",
       "      <td>MIV share (modal split) [%]</td>\n",
       "      <td>share of motorised private transport in traffi...</td>\n",
       "      <td>1</td>\n",
       "      <td>83.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>60</td>\n",
       "      <td>miv_share</td>\n",
       "      <td>miv_share_modal_split</td>\n",
       "      <td>Winkel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>601</td>\n",
       "      <td>PW new registrations per 1000 inhabitants [amo...</td>\n",
       "      <td>number of newly registered cars</td>\n",
       "      <td>2</td>\n",
       "      <td>38.3</td>\n",
       "      <td>2005</td>\n",
       "      <td>59</td>\n",
       "      <td>amount_new_pw_registrations</td>\n",
       "      <td>amount_new_pw_registrations_per_1000_inhabitants</td>\n",
       "      <td>Wil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>606</td>\n",
       "      <td>Hybrid motor cars stock [%]</td>\n",
       "      <td>share of hybrid vehicles</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2009</td>\n",
       "      <td>128</td>\n",
       "      <td>share_hybrid_cars</td>\n",
       "      <td>share_of_hybrid_cars</td>\n",
       "      <td>Wildberg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>607</td>\n",
       "      <td>Electric motor cars stock [%]</td>\n",
       "      <td>share of electric vehicles</td>\n",
       "      <td>1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2019</td>\n",
       "      <td>48</td>\n",
       "      <td>share_electric_cars</td>\n",
       "      <td>share_electric_cars</td>\n",
       "      <td>Hoeri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>611</td>\n",
       "      <td>New registrations of hybrid motor cars [%]</td>\n",
       "      <td>share of hybrid vehicles within the newly regi...</td>\n",
       "      <td>1</td>\n",
       "      <td>4.2</td>\n",
       "      <td>2018</td>\n",
       "      <td>144</td>\n",
       "      <td>new_hybrid_car_registrations</td>\n",
       "      <td>new_hybrid_car_registrations</td>\n",
       "      <td>Dinhard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>612</td>\n",
       "      <td>New registrations electric motor cars [%]</td>\n",
       "      <td>share of electric vehicles within the newly re...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2003</td>\n",
       "      <td>61</td>\n",
       "      <td>new_electric_car_registrations</td>\n",
       "      <td>new_electric_car_registrations</td>\n",
       "      <td>Bachs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>996</td>\n",
       "      <td>Total new registrations of electric and hybri...</td>\n",
       "      <td>Total new registrations of electric and hybri...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2006</td>\n",
       "      <td>134</td>\n",
       "      <td>total_registrations_electric_hybrid_cars</td>\n",
       "      <td>total_reg_electric_cars</td>\n",
       "      <td>Moenchaltorf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>997</td>\n",
       "      <td>Total of electric and hybrid motor cars stock [%]</td>\n",
       "      <td>Total of electric and hybrid motor cars stock [%]</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2004</td>\n",
       "      <td>88</td>\n",
       "      <td>total_electric_hybrid_cars</td>\n",
       "      <td>total_electric_cars</td>\n",
       "      <td>Grueningen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>998</td>\n",
       "      <td>Charging stations of electronic cars per 1000 ...</td>\n",
       "      <td>Charging stations of electronic cars per 1000 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>26</td>\n",
       "      <td>charging_stations_electric_cars_per_inh</td>\n",
       "      <td>charging_stations_per_inh</td>\n",
       "      <td>Humlikon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>999</td>\n",
       "      <td>Charging stations of electronic cars [no.]</td>\n",
       "      <td>Charging stations of electronic cars [no.]</td>\n",
       "      <td>1</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2021</td>\n",
       "      <td>199</td>\n",
       "      <td>charging_stations_electric_cars</td>\n",
       "      <td>charging_stations</td>\n",
       "      <td>Horgen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    indicator_id                                     indicator_name  \\\n",
       "0            385                   Access by bus [% of inhabitants]   \n",
       "1            386        Access by suburban train [% of inhabitants]   \n",
       "2            399         Passenger cars per 1000 inhabitants [no.]    \n",
       "3            401  Accessibility by suburban train+bus [% of inha...   \n",
       "4            480          public transport share (modal split) [%]    \n",
       "5            481                       MIV share (modal split) [%]    \n",
       "6            601  PW new registrations per 1000 inhabitants [amo...   \n",
       "7            606                        Hybrid motor cars stock [%]   \n",
       "8            607                     Electric motor cars stock [%]    \n",
       "9            611        New registrations of hybrid motor cars [%]    \n",
       "10           612         New registrations electric motor cars [%]    \n",
       "11           996   Total new registrations of electric and hybri...   \n",
       "12           997  Total of electric and hybrid motor cars stock [%]   \n",
       "13           998  Charging stations of electronic cars per 1000 ...   \n",
       "14           999        Charging stations of electronic cars [no.]    \n",
       "\n",
       "                                    short_description  question_type  value  \\\n",
       "0    share of people living in proximity of a busstop              1    3.8   \n",
       "1   share of people living in proximity of a train...              1    7.9   \n",
       "2                           number of cars per capita              2  539.9   \n",
       "3   share of people living in proximity of a train...              1    0.0   \n",
       "4   share of public transport in traffic movements...              1   14.0   \n",
       "5   share of motorised private transport in traffi...              1   83.0   \n",
       "6                     number of newly registered cars              2   38.3   \n",
       "7                            share of hybrid vehicles              1    0.2   \n",
       "8                          share of electric vehicles              1    0.4   \n",
       "9   share of hybrid vehicles within the newly regi...              1    4.2   \n",
       "10  share of electric vehicles within the newly re...              1    0.0   \n",
       "11   Total new registrations of electric and hybri...              1    0.9   \n",
       "12  Total of electric and hybrid motor cars stock [%]              1    0.0   \n",
       "13  Charging stations of electronic cars per 1000 ...              1    0.0   \n",
       "14        Charging stations of electronic cars [no.]               1   15.0   \n",
       "\n",
       "    year  spatialunit_id                                 view_name  \\\n",
       "0   2009              19                         accessibility_bus   \n",
       "1   2014              89                       accessibility_train   \n",
       "2   2016             114                  number_of_passenger_cars   \n",
       "3   2001             146               accessibility_train_and_bus   \n",
       "4   2016             155                    public_transport_share   \n",
       "5   2013              60                                 miv_share   \n",
       "6   2005              59               amount_new_pw_registrations   \n",
       "7   2009             128                         share_hybrid_cars   \n",
       "8   2019              48                       share_electric_cars   \n",
       "9   2018             144              new_hybrid_car_registrations   \n",
       "10  2003              61            new_electric_car_registrations   \n",
       "11  2006             134  total_registrations_electric_hybrid_cars   \n",
       "12  2004              88                total_electric_hybrid_cars   \n",
       "13  2021              26   charging_stations_electric_cars_per_inh   \n",
       "14  2021             199           charging_stations_electric_cars   \n",
       "\n",
       "                                   view_column_value municipality_name  \n",
       "0                                      access_by_bus           Dachsen  \n",
       "1                           access_by_suburban_train            Hinwil  \n",
       "2                passenger_cars_per_1000_inhabitants      Uetikon a.S.  \n",
       "3                   access_by_suburban_train_and_bus   Ellikon a.d.Th.  \n",
       "4                 public_transport_share_modal_split           Seuzach  \n",
       "5                              miv_share_modal_split            Winkel  \n",
       "6   amount_new_pw_registrations_per_1000_inhabitants               Wil  \n",
       "7                               share_of_hybrid_cars          Wildberg  \n",
       "8                                share_electric_cars             Hoeri  \n",
       "9                       new_hybrid_car_registrations           Dinhard  \n",
       "10                    new_electric_car_registrations             Bachs  \n",
       "11                           total_reg_electric_cars      Moenchaltorf  \n",
       "12                               total_electric_cars        Grueningen  \n",
       "13                         charging_stations_per_inh          Humlikon  \n",
       "14                                 charging_stations            Horgen  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the vectors in the templates\n",
    "def create_question_query(sample: pd.Series) -> Tuple[List[str], List[str]]:\n",
    "    '''\n",
    "    From a sample values, create 7 questions and 7 associated queries\n",
    "    Args:\n",
    "        - sample: row of random observations values\n",
    "    Return:\n",
    "        - questions: list of question generated with the values from sample\n",
    "        - queries: : list of queries generated with the values from sample\n",
    "    '''\n",
    "\n",
    "    indicator, view, view_column, random_value, indicator_id, indicator_year, municipality = sample[\n",
    "        ['short_description', 'view_name', 'view_column_value', 'value', 'indicator_id', 'year', 'municipality_name']\n",
    "    ].values\n",
    "    \n",
    "    questions = [\n",
    "        f\"How high is the {indicator} in {municipality} in the year {indicator_year}?\", # 0\n",
    "        f\"Which municipality has the highest {indicator}?\", # 1\n",
    "        f\"Which municipality has the minimum {indicator}?\", # 2\n",
    "        f\"What are the highest, lowest and average {indicator}?\", # 3\n",
    "        f\"How many municipalities have a {indicator} higher than {random_value} per year?\", # 4\n",
    "        f\"How high is the total {indicator} in the Canton Zurich in the year {indicator_year}?\", # 5\n",
    "        f\"Which region had the lowest {indicator} in the year {indicator_year}?\" # 6\n",
    "    ]\n",
    "    \n",
    "    queries = [\n",
    "        # 0\n",
    "        f\"SELECT T1.{view_column} \\\n",
    "        FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T2.name LIKE '{municipality}' AND T1.year = {indicator_year}\", \n",
    "        \n",
    "        # 1\n",
    "        f\"SELECT T2.name \\\n",
    "        FROM spatialunit AS T2 \\\n",
    "        JOIN {view} AS T1 ON T2.spatialunit_id = T1.spatialunit_id \\\n",
    "        ORDER BY T1.{view_column} DESC LIMIT 1\",\n",
    "        \n",
    "        # 2\n",
    "        f\"SELECT T2.name \\\n",
    "        FROM spatialunit AS T2 \\\n",
    "        JOIN {view} AS T1 ON T2.spatialunit_id = T1.spatialunit_id \\\n",
    "        ORDER BY T1.{view_column} ASC LIMIT 1\",\n",
    "        \n",
    "        # 3\n",
    "        f\"SELECT MAX(T1.{view_column}::numeric), \\\n",
    "        MIN(T1.{view_column}::numeric), AVG(T1.{view_column}::numeric) \\\n",
    "        FROM {view} AS T1\",\n",
    "        \n",
    "        # 4\n",
    "        f\"SELECT T1.year, COUNT(*) \\\n",
    "        FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T1.{view_column}::numeric > {random_value} AND T2.type_id = 1 \\\n",
    "        GROUP BY T1.year\",\n",
    "        \n",
    "        # 5\n",
    "        f\"SELECT T1.{view_column} FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T2.type_id = 8 AND T1.year = {indicator_year}\",\n",
    "        \n",
    "        # 6\n",
    "        f\"SELECT T2.name \\\n",
    "        FROM {view} AS T1 \\\n",
    "        JOIN spatialunit AS T2 ON T1.spatialunit_id = T2.spatialunit_id \\\n",
    "        WHERE T2.type_id = 4 AND T1.year = {indicator_year} \\\n",
    "        ORDER BY T1.{view_column} \\\n",
    "        LIMIT 1\"\n",
    "    ]\n",
    "    return questions, queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the 7 questions and queries for each sample\n",
    "all_questions, all_queries = [], []\n",
    "for _, sample in samples.iterrows():   \n",
    "    questions, queries = create_question_query(sample)\n",
    "    all_questions.extend(questions)\n",
    "    all_queries.extend(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save questions and queries\n",
    "df = pd.DataFrame({'questions': all_questions, 'queries': all_queries})\n",
    "df.to_csv(\"questions_queries_python.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the SQL queries on Postgres (Optionnal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this if you want to test the queries in postgres (you will need an up to date postgres database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "NER_API_SECRET=%env NER_API_SECRET\n",
    "API_KEY=%env API_KEY\n",
    "DB_USER=%env DB_USER\n",
    "DB_PW=%env DB_PW\n",
    "DB_HOST=%env DB_HOST\n",
    "DB_PORT=%env DB_PORT\n",
    "DB_SCHEMA=\"public\"\n",
    "DB='hack_zurich'\n",
    "DRIVERNAME = \"postgresql\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting with engine Engine(postgresql://postgres:***@database-1.cluster-cuqkxqloyykq.eu-central-1.rds.amazonaws.com:5432/hack_zurich)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22988/3535624919.py:3: SADeprecationWarning: Calling URL() directly is deprecated and will be disabled in a future release.  The public constructor for URL is now the URL.create() method.\n",
      "  sqlalchemy.engine.url.URL(\n"
     ]
    }
   ],
   "source": [
    "# Connection with sqalchemy database\n",
    "engine = sqlalchemy.create_engine(\n",
    "    sqlalchemy.engine.url.URL(\n",
    "        drivername=DRIVERNAME,\n",
    "        username=DB_USER,\n",
    "        password=DB_PW,\n",
    "        host=DB_HOST,\n",
    "        port=DB_PORT,\n",
    "        database=DB,\n",
    "    ),\n",
    "    echo_pool=True,\n",
    ")\n",
    "print(\"connecting with engine \" + str(engine))\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv containing the queries\n",
    "df = pd.read_csv(\"questions_queries_python.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(df: pd.DataFrame, query_number: int) -> None:\n",
    "    '''\n",
    "    Run a query on the database and prints the associted question and answer\n",
    "    Args:\n",
    "        - df: dataframe with all random samples\n",
    "        - query_number: index of the row of (question, query) to select\n",
    "    '''\n",
    "    question = df['questions'].iloc[query_number]\n",
    "    query = df['queries'].iloc[query_number]\n",
    "    answer = connection.execute(query)\n",
    "    \n",
    "    print(f\"Question {query_number}: {question}\")\n",
    "    print(f\"Answer {query_number}: {[r for r in answer]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run one query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6: Which region had the lowest share of people living in proximity of a busstop in the year 2014?\n",
      "Answer 6: [('Region Zimmerberg',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_query(df, query_number=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run all queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0: How high is the share of people living in proximity of a busstop in Dorf in the year 2014?\n",
      "Answer 0: [('90.5',)]\n",
      "\n",
      "Question 1: Which municipality has the highest share of people living in proximity of a busstop?\n",
      "Answer 1: [('Faellanden',)]\n",
      "\n",
      "Question 2: Which municipality has the minimum share of people living in proximity of a busstop?\n",
      "Answer 2: [('Andelfingen',)]\n",
      "\n",
      "Question 3: What are the highest, lowest and average share of people living in proximity of a busstop?\n",
      "Answer 3: [(Decimal('99.6'), Decimal('0'), Decimal('54.4117080745341615'))]\n",
      "\n",
      "Question 4: How many municipalities have a share of people living in proximity of a busstop higher than 90.5 per year?\n",
      "Answer 4: [(2013, 26), (2003, 20), (2015, 26), (2008, 20), (2014, 25), (2010, 23), (2007, 17), (2002, 20), (2004, 20), (2006, 16), (2000, 17), (2011, 26), (2001, 18), (2012, 27), (2009, 19), (2005, 21)]\n",
      "\n",
      "Question 5: How high is the total share of people living in proximity of a busstop in the Canton Zurich in the year 2014?\n",
      "Answer 5: [('48.3',)]\n",
      "\n",
      "Question 6: Which region had the lowest share of people living in proximity of a busstop in the year 2014?\n",
      "Answer 6: [('Region Zimmerberg',)]\n",
      "\n",
      "Question 7: How high is the share of people living in proximity of a trainstation in Elgg (bis 2017) in the year 2015?\n",
      "Answer 7: [('10',)]\n",
      "\n",
      "Question 8: Which municipality has the highest share of people living in proximity of a trainstation?\n",
      "Answer 8: [('Thalheim a.d.Th.',)]\n",
      "\n",
      "Question 9: Which municipality has the minimum share of people living in proximity of a trainstation?\n",
      "Answer 9: [('Freienstein-Teufen',)]\n",
      "\n",
      "Question 10: What are the highest, lowest and average share of people living in proximity of a trainstation?\n",
      "Answer 10: [(Decimal('85.2'), Decimal('0'), Decimal('6.2056211180124224'))]\n",
      "\n",
      "Question 11: How many municipalities have a share of people living in proximity of a trainstation higher than 10.0 per year?\n",
      "Answer 11: [(2013, 23), (2003, 32), (2015, 23), (2008, 28), (2014, 21), (2010, 23), (2007, 31), (2002, 32), (2004, 31), (2006, 32), (2000, 34), (2011, 25), (2001, 39), (2012, 22), (2009, 27), (2005, 32)]\n",
      "\n",
      "Question 12: How high is the total share of people living in proximity of a trainstation in the Canton Zurich in the year 2015?\n",
      "Answer 12: [('3.8',)]\n",
      "\n",
      "Question 13: Which region had the lowest share of people living in proximity of a trainstation in the year 2015?\n",
      "Answer 13: [('Region Zuerich',)]\n",
      "\n",
      "Question 14: How high is the number of cars per capita in Zell in the year 2015?\n",
      "Answer 14: [('533.7',)]\n",
      "\n",
      "Question 15: Which municipality has the highest number of cars per capita?\n",
      "Answer 15: [('Dielsdorf',)]\n",
      "\n",
      "Question 16: Which municipality has the minimum number of cars per capita?\n",
      "Answer 16: [('Rickenbach',)]\n",
      "\n",
      "Question 17: What are the highest, lowest and average number of cars per capita?\n",
      "Answer 17: [(Decimal('805.1'), Decimal('182.8'), Decimal('563.4521439915299100'))]\n",
      "\n",
      "Question 18: How many municipalities have a number of cars per capita higher than 533.7 per year?\n",
      "Answer 18: [(2013, 140), (2003, 110), (2015, 135), (2008, 122), (2014, 138), (2010, 130), (2019, 123), (2007, 124), (2002, 108), (2004, 121), (2006, 125), (2020, 122), (2011, 132), (2017, 133), (2016, 135), (2012, 139), (2018, 129), (2009, 126), (2005, 115)]\n",
      "\n",
      "Question 19: How high is the total number of cars per capita in the Canton Zurich in the year 2015?\n",
      "Answer 19: [('488.6',)]\n",
      "\n",
      "Question 20: Which region had the lowest number of cars per capita in the year 2015?\n",
      "Answer 20: [('Region Zuerich',)]\n",
      "\n",
      "Question 21: How high is the share of people living in proximity of a trainstation or busstop in Buch a.I. in the year 2013?\n",
      "Answer 21: [('0',)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    run_query(df, i)\n",
    "    if i>20:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"questions_queries_python.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199480/1777422178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ML-Model taken from https://github.com/Vamsi995/Paraphrase-Generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# https://huggingface.co/Vamsi/T5_Paraphrase_Paws\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vamsi/T5_Paraphrase_Paws\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Vamsi/T5_Paraphrase_Paws\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1717\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"loading file {file_path} from cache at {resolved_vocab_files[file_id]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1719\u001b[0;31m         return cls._from_pretrained(\n\u001b[0m\u001b[1;32m   1720\u001b[0m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1790\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1791\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1792\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1793\u001b[0m             raise OSError(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, tokenizer_file, eos_token, unk_token, pad_token, extra_ids, additional_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    126\u001b[0m                 )\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mvocab_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mtokenizer_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_slow_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslow_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    106\u001b[0m                 \u001b[0;34m\"Couldn't instantiate the backend tokenizer from one of: \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m\"(1) a `tokenizers` library serialization file, \\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Couldn't instantiate the backend tokenizer from one of: \n(1) a `tokenizers` library serialization file, \n(2) a slow tokenizer instance to convert or \n(3) an equivalent slow tokenizer class to instantiate and convert. \nYou need to have sentencepiece installed to convert a slow tokenizer to a fast one."
     ]
    }
   ],
   "source": [
    "# ML-Model taken from https://github.com/Vamsi995/Paraphrase-Generator\n",
    "# https://huggingface.co/Vamsi/T5_Paraphrase_Paws\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_paraphrases(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Script for creating paraphrases and thus expanding the q&a pairs generated through the first script\n",
    "    At the time of writing it generates from 90 initial q&a pairs, around \n",
    "\n",
    "    ML-Model taken from https://github.com/Vamsi995/Paraphrase-Generator\n",
    "    https://huggingface.co/Vamsi/T5_Paraphrase_Paws\n",
    "\n",
    "    Version 0.1.1 - 15.09.2021\n",
    "    Christian Ruiz - Statistisches Amt Kanton Zürich\n",
    "    CC0\n",
    "    \n",
    "    History \n",
    "    Version 0.1.2 -15.09.2021 - Umlaut-corrections for the SQL-queries\n",
    "    Version 0.1.1 -15.09.2021 - First version public\n",
    "    '''\n",
    "    output_df = df\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        sentence=df['questions'].iloc[i]\n",
    "        print(i, \" \", sentence)\n",
    "        text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "        encoding = tokenizer.encode_plus(text, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "        input_ids, attention_masks = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
    "\n",
    "        outputs = model.generate(\n",
    "            input_ids=input_ids, attention_mask=attention_masks,\n",
    "            max_length=256,\n",
    "            do_sample=True,\n",
    "            top_k=120,\n",
    "            top_p=0.95,\n",
    "            early_stopping=True,\n",
    "            num_return_sequences=8\n",
    "        )\n",
    "\n",
    "        for output in outputs:\n",
    "            line = tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "            output_df = output_df.append({'questions': line, 'queries': df['queries'].iloc[i]}, ignore_index=True)\n",
    "    \n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_paraphrases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_199480/780167365.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_paraphrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_paraphrases' is not defined"
     ]
    }
   ],
   "source": [
    "output_df = generate_paraphrases(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjustments\n",
    "output_df = output_df[['questions','queries']]\n",
    "output_df['questions'] = output_df['questions'].str.lower()\n",
    "output_df = output_df.drop_duplicates()\n",
    "\n",
    "# Replace the Umlaut in the SQL-queries for ValueNet\n",
    "output_df['queries'] = output_df['queries'].str.replace(\"ü\", \"ue\")\n",
    "output_df['queries'] = output_df['queries'].str.replace(\"ä\", \"ae\")\n",
    "output_df['queries'] = output_df['queries'].str.replace(\"ö\", \"oe\")\n",
    "\n",
    "# Save paraphrases\n",
    "output_df.to_csv(\"questions_queries_paraphrases.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to valunet preprocessing format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valuenet errors when tokenising the SQL with `::numeric`, hence we remove it here. \n",
    "If you query a PostgreSQL database again with these queries, be mindful that you might need the `::numeric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in csv containing the queries\n",
    "#df = pd.read_csv(\"questions_queries_python.csv\")\n",
    "df = pd.read_csv(\"questions_queries_paraphrases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_valuenet_format(df: pd.DataFrame) -> list:\n",
    "    handmade_data = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        query = row['queries'].replace('::numeric', '')\n",
    "        row_dict = {\n",
    "            'db_id': 'hack_zurich',\n",
    "            'query': query,\n",
    "            'question': row['questions']\n",
    "        }\n",
    "        handmade_data.append(row_dict)\n",
    "    return handmade_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "handmade_data = df_to_valuenet_format(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save question_queries in the format required by valuenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "training_samples = int(len(handmade_data)*TRAIN_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = handmade_data[:training_samples]\n",
    "dev_data = handmade_data[training_samples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('handmade_data_train.json', 'w') as outfile:\n",
    "    json.dump(train_data, outfile)\n",
    "\n",
    "with open('handmade_data_dev.json', 'w') as outfile:\n",
    "    json.dump(dev_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan to train Valuenet with this data, you can follow the steps explained in [preprocess_custom_data-01.ipynb](https://github.com/hack-with-admin-ch/aws-sagemaker-notebook-valuenet/blob/training_options/preprocess_custom_data-01.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
